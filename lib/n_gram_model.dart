import 'package:taco/list_keyed_map_utilities.dart';

/// A class that generates all possible n-grams (subsequences of `n` tokens) from a list of input texts.
///
/// In natural language processing, n-grams are contiguous sequences of n items from a given input text. The items
/// can be words, characters, or any other units that make sense in the context of the problem being solved.
///
/// For example, in the sentence "The quick brown fox jumps over the lazy dog", some possible 2-grams (also
/// called bigrams) are "The quick", "quick brown", "brown fox", "fox jumps", "jumps over", "over the", "the lazy",
/// and "lazy dog". Similarly, some possible 3-grams (also called trigrams) are "The quick brown", "quick brown fox",
/// "brown fox jumps", "fox jumps over", "jumps over the", "over the lazy", and "the lazy dog".

/// N-grams are useful in natural language processing because they can be used to extract patterns and relationships
/// between words in a text. For example, the frequency of certain n-grams can be used to predict the likelihood of
/// certain words following other words, which is useful in tasks like text completion and language modeling.

/// N-grams can be generated from a text by sliding a "window" of length n over the text and extracting the sequence of
/// items within the window at each position. The resulting sequences can be stored in a list, set, or other data
/// structure and used for further analysis. In some cases, n-grams can also be generated by using regular expressions
/// or other more complex algorithms that take into account the structure of the input text.

class NGramModel {
  /// The size of the n-grams used for prediction.
  final int n;

  /// A mapping of n-grams to their associated next words and their counts.
  final Map<List<String>, Map<String, int>> ngrams;

  /// Creates an `NGramModel` with the given `n` parameter and a `corpus` of text.
  NGramModel({required this.n, required List<String> corpus})
      : ngrams = _generateNGrams(n, corpus);

  /// Generates n-grams and their associated next words and counts from the `corpus`.
  ///
  /// This method takes the `n` parameter and a list of `corpus` words and returns
  /// a mapping of n-grams to their associated next words and counts. The method
  /// uses a sliding window of size `n` over the `corpus` and generates the
  /// n-grams from the resulting context and next word. The counts of each next
  /// word are stored in the resulting mapping for each n-gram.
  static Map<List<String>, Map<String, int>> _generateNGrams(
      int n, List<String> corpus) {
    final ngrams = <List<String>, Map<String, int>>{};

    for (int i = 0; i < corpus.length - n; i++) {
      final context = corpus.sublist(i, i + n - 1);
      final word = corpus[i + n];

      if (!ngrams.containsKey(context)) {
        ngrams[context] = {};
      }

      if (!ngrams[context]!.containsKey(word)) {
        ngrams[context]![word] = 0;
      }

      ngrams[context]![word] = ngrams[context]![word]! + 1;
    }

    return ngrams;
  }

  /// Returns the predicted next word for the given `word` and `context`.
  ///
  /// The `context` parameter is a list of the previous `n-1` words forming the
  /// context for the prediction. The method returns the most likely next word
  /// based on the n-grams generated during training. If no prediction is
  /// available for the context, an empty string is returned.
  String predictNextWord({required List<String> context}) {
    final List<String> contextNgram =
        context.sublist(context.length - n + 1, context.length);

    Map<String, int>? possibleNextWords = ngrams.getByListKey(contextNgram);

    if (possibleNextWords == null) {
      print('No prediction available');
      return '';
    }

    final nextWord = possibleNextWords.keys.reduce(
        (a, b) => possibleNextWords[a]! > possibleNextWords[b]! ? a : b);

    return nextWord;
  }
}
